# -*- coding: utf-8 -*-
"""IBM-HR Data Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/ibm-hr-data-analysis-d235aac7-73a3-4802-8f0f-920b7dd64080.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240511/auto/storage/goog4_request%26X-Goog-Date%3D20240511T114742Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da598949af73479fdf19be3cc9f289500408b35357ae11d5fde2bdd55cdd815021e33cd2e20f62b033f1343128528fb16da4ab0ac631508d6f38fdc86e43c90f7110ed0c6d6f4a5f7801f83f2d20411218eb465788e33e4f450d2a1437683af33a16c670bd6300a5f2102237830875e7d0dcdd77af473a247c51c9eb5c800f05cef26bbc45cde4cac110ca6c8a1d8ad0bf8e785757f29d98654a635d734ae0cbf0e9e709ab45265cfe9fae228e4ebb4e1fb1582025711a7b7657f1c06a33aeee9931beac9dd109eb7c591d12f9b26d9c99e93bc33f0ab492df3992be6cd621aa0b27cefdf5d52603af049f08e902c872fd49311d9c46771150bda1b2a42934b0a

<div style="text-align: center; background-color: #8ABEB9; padding: 10px;">
    <h2 style="font-weight: bold;">HR ANALYTICS OUTLINE</h2>
</div>

About Dataset: https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset
- Importing Libraries
- Loading Dataset
- Exploratory Data Analysis
- Label Encoding
- Data Processing
- Train and Test Dataset
- Data Modeling
   - 1] Logistic Regression
   - 2] Random Forest
   - 3] Support Vector Machine
   - 4] XGBOOST
   - 5] ADABOOST
- Comparing Model Performance

#Importing Libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
sns.set_context("notebook")

# %matplotlib inline
sns.set_style("whitegrid")
plt.style.use("fivethirtyeight")

pd.set_option('display.max_columns', None)

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, precision_recall_curve, roc_curve
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier

"""#Loading Dataset"""

employee_data = pd.read_csv('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv')

employee_data.head(5)

employee_data.tail(5)

"""## **Summary Statistics of numeric variables**:"""

employee_data.describe().transpose().round(2)

employee_data.describe(include="all")

"""## **Summary Statistics of categorical variables**:"""

# Get the list of categorical columns
cat_cols = employee_data.select_dtypes(include='object').columns.tolist()

# Create a DataFrame containing counts of unique values for each categorical column
cat_employee_data = pd.DataFrame(employee_data[cat_cols].melt(var_name='column', value_name='value')
                      .value_counts()).rename(columns={0: 'count'}).sort_values(by=['column', 'count'])

# Display summary statistics of categorical variables
display(employee_data[cat_cols].describe())

# Display counts of unique values for each categorical column
display(cat_employee_data)

employee_data.describe(include='O')

"""## **Dataset Size**:"""

employee_data.shape

"""## **List of Columns**:"""

employee_data.columns

num_rows, num_cols = employee_data.shape
print(f'Number of rows: {num_rows}\nNumber of columns: {num_cols}')

"""## **Data Information**:"""

employee_data.info()

numerical_vars = employee_data.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_vars = employee_data.select_dtypes(include=['object']).columns.tolist()
print('Numerical variables:', numerical_vars)
print('Categorical variables:', categorical_vars)

# Count the number of categorical and numerical variables
categorical_count = employee_data.select_dtypes(include='object').shape[1]
numerical_count = employee_data.select_dtypes(exclude='object').shape[1]

print(f"Number of categorical variables: {categorical_count}")
print(f"Number of numerical variables: {numerical_count}")

# Counts of unique values for each categorical column
cat_unique_counts = employee_data[cat_cols].nunique().reset_index().rename(columns={'index': 'column', 0: 'unique_count'})

# Counts of non-unique values for each categorical column
cat_non_unique_counts = employee_data[cat_cols].apply(lambda x: x.shape[0] - x.nunique()).reset_index().rename(columns={'index': 'column', 0: 'non_unique_count'})

# Merging unique and non-unique counts
cat_counts = pd.merge(cat_unique_counts, cat_non_unique_counts, on='column')
print("\nCounts of Unique and Non-Unique Values for Categorical Variables:")
print(cat_counts)

# Inspect useless features
employee_data.nunique().sort_values()

"""## **Duplicate Records**:"""

employee_data[employee_data.duplicated(keep=False)]

"""No Duplicate Records present in the Dataset."""

employee_data=employee_data.drop_duplicates()
employee_data.shape

"""## **Missing Values**:"""

missing_employee_data = employee_data.isnull().sum().to_frame().rename(columns={0:"Total No. of Missing Values"})
missing_employee_data["% of Missing Values"] = round((missing_employee_data["Total No. of Missing Values"]/len(employee_data))*100,2)
missing_employee_data

"""# Exploratory Data Analysis and Data Visualization"""

# Correlation matrix

# Select only the numeric columns from the DataFrame
numeric_columns = employee_data.select_dtypes(include=['number'])

# Calculate the correlation matrix
correlation_matrix = numeric_columns.corr()

# Create a heatmap to visualize the correlations
plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Heatmap Plotting
# Select only numeric columns
numeric_columns = employee_data.select_dtypes(include=['int64', 'float64'])

# Calculate the correlation matrix
corr_matrix = numeric_columns.corr()

# Filter correlation matrix to include values greater than 0.5 or less than -0.5
corr_matrix_filtered = corr_matrix[(corr_matrix > 0.5) | (corr_matrix < -0.5)]

# Plot the heatmap with filtered correlation values
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix_filtered, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Numeric Features (|Correlation| > 0.5)')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=employee_data, x='Age', hue='Attrition', kde=True)
plt.title('Age Distribution by Attrition')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=employee_data, x='Gender', hue='Attrition')
plt.title('Attrition by Gender')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(data=employee_data, x='JobRole', hue='Attrition', palette='viridis')
plt.title('Attrition by Job Role')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=employee_data, x='Education', hue='Attrition', palette='Set2')
plt.title('Attrition by Education Level')
plt.xlabel('Education Level')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.kdeplot(data=employee_data, x='DistanceFromHome', hue='Attrition', fill=True, common_norm=False, palette='husl')
plt.title('Attrition by Distance from Home')
plt.xlabel('Distance from Home (Miles)')
plt.ylabel('Density')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=employee_data, x='MonthlyIncome', hue='Attrition', kde=True, palette='YlGnBu')
plt.title('Monthly Income Distribution by Attrition')
plt.xlabel('Monthly Income')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=employee_data, x='MaritalStatus', y='Age', hue='Attrition', palette='coolwarm')
plt.title('Attrition by Marital Status and Age')
plt.xticks(rotation=90)
plt.xlabel('Marital Status')
plt.ylabel('Age')
plt.show()

"""### **Checking Unique Value Of Categorical Attributes**"""

# Calculate the number of unique values in each column
for column in employee_data.columns:
    print(f"{column} - Number of unique values : {employee_data[column].nunique()}")
    print("------------------------------------------------------------------------")

categorical_features = []
for column in employee_data.columns:
    if employee_data[column].dtype == object and len(employee_data[column].unique()) <= 30:
        categorical_features.append(column)
        print(f"{column} : {employee_data[column].unique()}")
        print(employee_data[column].value_counts())
        print("====================================================================================")
categorical_features.remove('Attrition')

# Get the value counts for the 'Attrition' column
value_counts = employee_data['Attrition'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Get the value counts for the 'BusinessTravel' column
value_counts = employee_data['BusinessTravel'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Get the value counts for the 'Department' column
value_counts = employee_data['Department'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Get the value counts for the 'EducationField' column
value_counts = employee_data['EducationField'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Get the value counts for the 'Gender' column
value_counts = employee_data['Gender'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Get the value counts for the 'JobRole' column
value_counts = employee_data['JobRole'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Get the value counts for the 'MaritalStatus' column
value_counts = employee_data['MaritalStatus'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Get the value counts for the 'OverTime' column
value_counts = employee_data['OverTime'].value_counts()

# Calculate the percentage
percentage = (value_counts / len(employee_data)) * 100

# Concatenate the count and percentage into a DataFrame
count_and_percentage = pd.concat([value_counts, percentage], axis=1)
count_and_percentage.columns = ['Count', 'Percentage']

# Round the values in the DataFrame to two decimal places
count_and_percentage_rounded = count_and_percentage.round(2)

# Calculate the total count
total_count = len(employee_data)

# Create a DataFrame for total count
total_employee_data = pd.DataFrame({'Count': [total_count], 'Percentage': [100]}, index=['Total'])

# Concatenate the total count DataFrame with the counts and percentages DataFrame
result_employee_data = pd.concat([count_and_percentage_rounded, total_employee_data])

# Display the result DataFrame
print(result_employee_data)

# Print the shape of the DataFrame
print("The shape of data frame:", employee_data.shape)
# Print the length (number of rows) of the DataFrame
print("Number of Rows in the dataframe:", len(employee_data))
# Print the number of columns in the DataFrame
print("Number of Columns in the dataframe:", len(employee_data.columns))

"""#Encoding of Dataset"""

# Convert categorical variables into numerical form.
label = LabelEncoder()
employee_data["Attrition"] = label.fit_transform(employee_data.Attrition)

employee_data.info()

"""#Data Preprocessing"""

# Transform categorical data into dummies
dummy_col = [column for column in employee_data.drop('Attrition', axis=1).columns if employee_data[column].nunique() < 20]
data = pd.get_dummies(employee_data, columns=dummy_col, drop_first=True, dtype='uint8')
data.info()

print(data.shape)

# Remove duplicate Features
data = data.T.drop_duplicates()
data = data.T

# Remove Duplicate Rows
data.drop_duplicates(inplace=True)

print(data.shape)

data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values().plot(kind='barh', figsize=(10, 30))

data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values().plot(kind='barh', figsize=(10, 30))

feature_correlation = data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values()
model_col = feature_correlation[np.abs(feature_correlation) > 0.02].index
len(model_col)

"""#Splitting the Dataset"""

X = data.drop('Attrition', axis=1)
y = data.Attrition

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,
                                                    stratify=y)

scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)
X_std = scaler.transform(X)

def feature_imp(df, model):
    fi = pd.DataFrame()
    fi["feature"] = df.columns
    fi["importance"] = model.feature_importances_
    return fi.sort_values(by="importance", ascending=False)

y_test.value_counts()[0] / y_test.shape[0]

"""#Showing the distribution of Attrition After splitting"""

stay = (y_train.value_counts()[0] / y_train.shape)[0]
leave = (y_train.value_counts()[1] / y_train.shape)[0]

print("===============TRAIN=================")
print(f"Staying Rate: {stay * 100:.2f}%")
print(f"Leaving Rate: {leave * 100 :.2f}%")

stay = (y_test.value_counts()[0] / y_test.shape)[0]
leave = (y_test.value_counts()[1] / y_test.shape)[0]

print("===============TEST=================")
print(f"Staying Rate: {stay * 100:.2f}%")
print(f"Leaving Rate: {leave * 100 :.2f}%")

def evaluate(model, X_train, X_test, y_train, y_test):
    y_test_pred = model.predict(X_test)
    y_train_pred = model.predict(X_train)

    print("TRAINIG RESULTS: \n===============================")
    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))
    print(f"CONFUSION MATRIX:\n{confusion_matrix(y_train, y_train_pred)}")
    print(f"ACCURACY SCORE:\n{accuracy_score(y_train, y_train_pred):.4f}")
    print(f"CLASSIFICATION REPORT:\n{clf_report}")

    print("TESTING RESULTS: \n===============================")
    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))
    print(f"CONFUSION MATRIX:\n{confusion_matrix(y_test, y_test_pred)}")
    print(f"ACCURACY SCORE:\n{accuracy_score(y_test, y_test_pred):.4f}")
    print(f"CLASSIFICATION REPORT:\n{clf_report}")

"""

#Logistic Regression"""

lr_clf = LogisticRegression(solver='liblinear', penalty='l1')
lr_clf.fit(X_train_std, y_train)

evaluate(lr_clf, X_train_std, X_test_std, y_train, y_test)

def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
    plt.plot(thresholds, recalls[:-1], "g--", label="Recall")
    plt.xlabel("Threshold")
    plt.legend(loc="upper left")
    plt.title("Precision/Recall Tradeoff")


def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], "k--")
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')


precisions, recalls, thresholds = precision_recall_curve(y_test, lr_clf.predict(X_test_std))
plt.figure(figsize=(14, 25))
plt.subplot(4, 2, 1)
plot_precision_recall_vs_threshold(precisions, recalls, thresholds)

plt.subplot(4, 2, 2)
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.title("PR Curve: precisions/recalls tradeoff");

plt.subplot(4, 2, 3)
fpr, tpr, thresholds = roc_curve(y_test, lr_clf.predict(X_test_std))
plot_roc_curve(fpr, tpr)

scores_dict = {
    'Logistic Regression': {
        'Train': roc_auc_score(y_train, lr_clf.predict(X_train)),
        'Test': roc_auc_score(y_test, lr_clf.predict(X_test)),
    },
}

# Get feature coefficients from the logistic regression model
feature_importance = np.abs(lr_clf.coef_[0])

# Create DataFrame for feature importance
df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance})

# Sort DataFrame by importance in descending order
df = df.sort_values(by='importance', ascending=False)

# Select top 40 features
df = df[:40]

# Plotting top 40 feature importance
plt.figure(figsize=(10, 10))
df.set_index('feature').plot(kind='barh', figsize=(10, 10))
plt.title('Feature Importance according to Logistic Regression')
plt.xlabel('Importance')
plt.show()

"""#Random Forest Clasifier"""

rf_clf = RandomForestClassifier(n_estimators=100, bootstrap=False,
#                                      class_weight={0:stay, 1:leave}
                                    )
rf_clf.fit(X_train, y_train)
evaluate(rf_clf, X_train, X_test, y_train, y_test)

scores_dict['Random Forest'] = {
        'Train': roc_auc_score(y_train, rf_clf.predict(X_train)),
        'Test': roc_auc_score(y_test, rf_clf.predict(X_test)),
    }

df = feature_imp(X, rf_clf)[:40]
df.set_index('feature', inplace=True)
df.plot(kind='barh', figsize=(10, 10))
plt.title('Feature Importance according to Random Forest')

"""
#Support Vector Machine"""

svm_clf = SVC(kernel='linear')
svm_clf.fit(X_train_std, y_train)

evaluate(svm_clf, X_train_std, X_test_std, y_train, y_test)

svm_clf = SVC(random_state=42)

param_grid = [
    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
    {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}
]

search = GridSearchCV(svm_clf, param_grid=param_grid, scoring='roc_auc', cv=3, refit=True, verbose=1)
search.fit(X_train_std, y_train)

svm_clf = SVC(**search.best_params_)
svm_clf.fit(X_train_std, y_train)

evaluate(svm_clf, X_train_std, X_test_std, y_train, y_test)

precisions, recalls, thresholds = precision_recall_curve(y_test, svm_clf.predict(X_test_std))
plt.figure(figsize=(14, 25))
plt.subplot(4, 2, 1)
plot_precision_recall_vs_threshold(precisions, recalls, thresholds)

plt.subplot(4, 2, 2)
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.title("PR Curve: precisions/recalls tradeoff");

plt.subplot(4, 2, 3)
fpr, tpr, thresholds = roc_curve(y_test, svm_clf.predict(X_test_std))
plot_roc_curve(fpr, tpr)

scores_dict['Support Vector Machine'] = {
        'Train': roc_auc_score(y_train, svm_clf.predict(X_train_std)),
        'Test': roc_auc_score(y_test, svm_clf.predict(X_test_std)),
    }

"""

#XGBoost Classifier."""

xgb_clf = XGBClassifier()
xgb_clf.fit(X_train, y_train)

evaluate(xgb_clf, X_train, X_test, y_train, y_test)

scores_dict['XGBoost'] = {
        'Train': roc_auc_score(y_train, xgb_clf.predict(X_train)),
        'Test': roc_auc_score(y_test, xgb_clf.predict(X_test)),
    }

precisions, recalls, thresholds = precision_recall_curve(y_test, xgb_clf.predict(X_test))
plt.figure(figsize=(14, 25))
plt.subplot(4, 2, 1)
plot_precision_recall_vs_threshold(precisions, recalls, thresholds)

plt.subplot(4, 2, 2)
plt.plot(precisions, recalls)
plt.xlabel("Precision")
plt.ylabel("Recall")
plt.title("PR Curve: precisions/recalls tradeoff");

plt.subplot(4, 2, 3)
fpr, tpr, thresholds = roc_curve(y_test, xgb_clf.predict(X_test))
plot_roc_curve(fpr, tpr)

df = feature_imp(X, xgb_clf)[:35]
df.set_index('feature', inplace=True)
df.plot(kind='barh', figsize=(10, 8))
plt.title('Feature Importance according to XGBoost')

"""<div style="text-align: center; background-color: #8ABEB9; padding: 10px;">
    <h2 style="font-weight: bold;">COMPARING MODEL PERFORMANCE</h2>
</div>
"""

ml_models = {
    'Random Forest': rf_clf,
    'XGBoost': xgb_clf,
    'Logistic Regression': lr_clf,
    'Support Vector Machine': svm_clf,
}

for model in ml_models:
    print(f"{model.upper():{30}} roc_auc_score: {roc_auc_score(y_test, ml_models[model].predict(X_test)):.3f}")

plt.figure(figsize=(10, 8))

# Iterate through each model
for model_name, model in ml_models.items():
    if hasattr(model, "predict_proba"):  # Check if the model has predict_proba method
        # Predict probabilities for positive class
        y_pred_prob = model.predict_proba(X_test)[:, 1]
    else:
        # For models without predict_proba, use decision_function or predict
        y_pred_prob = model.decision_function(X_test) if hasattr(model, "decision_function") else model.predict(X_test)

    # Compute ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

    # Compute AUC score
    auc_score = roc_auc_score(y_test, y_pred_prob)

    # Plot ROC curve
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend()
plt.grid(True)
plt.tight_layout()

plt.show()

"""To determine which model is better, we typically look at the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). A higher AUC value indicates better performance of the model in distinguishing between classes.

Based on the provided AUC values:

#### XGBoost (AUC=0.767)
#### Random Forest (AUC=0.736)
#### Logistic Regression (AUC=0.566)
#### Support Vector Machine (AUC=0.500)

From these values, we can see that AdaBoost has the highest AUC, followed by XGBoost and Random Forest. Therefore, based solely on the AUC values, AdaBoost would be considered the best-performing model among those listed.
AdaBoost,XGBoost,and Random Forest models have higher AUC scores, indicating better performance in distinguishing between positive and negative instances. Higher AUC scores suggest that the model has a better ability to rank positive instances higher than negative instances across different threshold values.
"""

